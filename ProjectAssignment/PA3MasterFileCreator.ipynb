{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ebe593-4a1f-4720-92c9-6542f451c310",
   "metadata": {},
   "source": [
    "### Setup\n",
    "----\n",
    "##### !! NOTE: this is for the creaton of the master file for project assignment 3 - sok1005!!\n",
    "To run this code you need the data from [chicagobooth](https://www.chicagobooth.edu/research/kilts/datasets/dominicks), specifically the data for shampoo.\n",
    "\n",
    "The filles needed is:\n",
    "\n",
    "**Customoer Count File** - (ccount(stata).zip)\n",
    "\n",
    "**Store-Level Demographics File** - (demo(stata).zip)\n",
    "\n",
    "And from the **category file** you need to find **shampoo**, and download **UPC.csv File** and **Movement.csv File**\n",
    "\n",
    "And all of this needs to be saved in a folder called **\"data\"** in the same folder you are running this script from, running the **\"create_folders\" function** will automatically create the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8ab07335-29f6-4ddb-923c-4c86f659ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import chardet\n",
    "import time\n",
    "import csv\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import sympy as sp\n",
    "from sympy.solvers import solve\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from cycler import cycler\n",
    "\n",
    "# custome plot style\n",
    "params  = {\n",
    "\"lines.linewidth\": 1.5,\n",
    "\n",
    "\"legend.fancybox\": \"true\",\n",
    "\n",
    "\"axes.prop_cycle\": cycler('color', [\"#ffa822\",\"#1ac0c6\",\"#ff6150\",\"#30B66A\",\"#B06AFF\",\"#FF21E1\"]),\n",
    "\"axes.facecolor\": \"#2b2b2b\",\n",
    "\"axes.axisbelow\": \"true\",\n",
    "\"axes.grid\": \"true\",\n",
    "\"axes.edgecolor\": \"#2b2b2b\",\n",
    "\"axes.linewidth\": 0.5,\n",
    "\"axes.labelpad\": 0,\n",
    "\n",
    "\"patch.edgecolor\": \"#2b2b2b\",\n",
    "\"patch.linewidth\": 0.5,\n",
    "\n",
    "\"grid.linestyle\": \"--\",\n",
    "\"grid.linewidth\": 0.5,\n",
    "\"grid.color\": \"#b8aba7\",\n",
    "\n",
    "\"xtick.major.size\": 0,\n",
    "\"xtick.minor.size\": 0,\n",
    "\"ytick.major.size\": 0,\n",
    "\"ytick.minor.size\": 0,\n",
    "\n",
    "\"font.family\":\"monospace\",\n",
    "\"font.size\":10.0,\n",
    "\"text.color\": \"#FFE9E3\",\n",
    "\"axes.labelcolor\": \"#b8aba7\",\n",
    "\"xtick.color\": \"#b8aba7\",\n",
    "\"ytick.color\": \"#b8aba7\",\n",
    "\n",
    "\"savefig.edgecolor\": \"#2b2b2b\",\n",
    "\"savefig.facecolor\": \"#2b2b2b\",\n",
    "\n",
    "\"figure.subplot.left\": 0.08,\n",
    "\"figure.subplot.right\": 0.95,\n",
    "\"figure.subplot.bottom\": 0.09,\n",
    "\"figure.facecolor\": \"#2b2b2b\"}\n",
    "\n",
    "pylab.rcParams.update(params)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3de8b6ce-a2ad-4486-9aeb-2e58c5dc5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect files from folder, if file type equals file_type\n",
    "def get_files(folder, file_type):\n",
    "    file_paths = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(file_type):\n",
    "            file_paths.append([os.path.join(folder, file), file_type])\n",
    "    return file_paths\n",
    "\n",
    "def get_encoder(file_path, chunksize = 10_000):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(chunksize))\n",
    "    return result['encoding']\n",
    "\n",
    "# NOTE TO SELF, STRAGE ERROR HERE, MIGHT BE A DUPLICATE ERROR\n",
    "# Load, filter in chunks and Convert to csv\n",
    "def load_and_filter_file(input_file, temp_path, filter_func:list, file_type=\".csv\", chunksize=10_000, new_file_name = \"\"):\n",
    "    temp_file = os.path.join(temp_path,new_file_name)\n",
    "    if new_file_name == \"\":\n",
    "        # Extract the file name from the input_file path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Create a temp_file path by combining temp_path and input_file_name\n",
    "        file_name_without_ext, file_ext = os.path.splitext(input_file_name)\n",
    "        temp_file = os.path.join(temp_path, f\"{file_name_without_ext}_temp.csv\")\n",
    "        \n",
    "    # Had to fix the decoding because 'invalid continuation byte' that utf-8 can't decode. And manual attempt to fix it did not reveal byte 0xd5\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"cp1252\", \"latin1\"]\n",
    "    success = False\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            if file_type.lower() == '.csv':\n",
    "                reader = pd.read_csv(input_file, chunksize=chunksize, encoding=encoding)\n",
    "            elif file_type.lower() == '.dta':\n",
    "                reader = pd.read_stata(input_file, chunksize=chunksize)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'dta'.\")\n",
    "\n",
    "            for i, chunk in enumerate(reader):\n",
    "                filtered_chunk = chunk\n",
    "                for func in filter_func:\n",
    "                    filtered_chunk = func(filtered_chunk)\n",
    "                if i == 0:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='w')\n",
    "                else:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='a', header=False)\n",
    "\n",
    "            success = True\n",
    "            print(f\"Succes with the encoding '{encoding}', file {temp_file} now created\")\n",
    "            break\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read the file with encoding '{encoding}', trying the next one...\")\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"None of the attempted encodings were successful in reading the file.\")\n",
    "        \n",
    "def load_and_filter_file_full_load(input_file, temp_path, filter_func:list, file_type=\".csv\", new_file_name = \"\"):\n",
    "    temp_file = os.path.join(temp_path,new_file_name)\n",
    "    if new_file_name == \"\":\n",
    "        # Extract the file name from the input_file path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Create a temp_file path by combining temp_path and input_file_name\n",
    "        file_name_without_ext, file_ext = os.path.splitext(input_file_name)\n",
    "        temp_file = os.path.join(temp_path, f\"{file_name_without_ext}_temp.csv\")\n",
    "        \n",
    "    # Had to fix the decoding because 'invalid continuation byte' that utf-8 can't decode. And manual attempt to fix it did not reveal byte 0xd5\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"cp1252\", \"latin1\"]\n",
    "    success = False\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            if file_type.lower() == '.csv':\n",
    "                df = pd.read_csv(input_file, encoding=encoding)\n",
    "            elif file_type.lower() == '.dta':\n",
    "                df = pd.read_stata(input_file)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'dta'.\")\n",
    "\n",
    "            for func in filter_func:\n",
    "                df = func(df)\n",
    "                \n",
    "            df.to_csv(temp_file, index=False)\n",
    "\n",
    "            success = True\n",
    "            print(f\"Succes with the encoding '{encoding}', file {temp_file} now created\")\n",
    "            break\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read the file with encoding '{encoding}', trying the next one...\")\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"None of the attempted encodings were successful in reading the file.\")\n",
    "\n",
    "            \n",
    "# Merge csv files\n",
    "def merge_csv_files(file1, file2, output_file, merge_on= None, merge_dtype=None, chunksize =10000):\n",
    "    if (merge_on is not None) and (type(merge_on) != list):  # Fix the condition here\n",
    "        merge_on = [merge_on]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = None\n",
    "        \n",
    "        # Read the entire file2 into memory\n",
    "        file2_data = pd.read_csv(file2)\n",
    "        \n",
    "        if merge_dtype is not None:\n",
    "            for column in merge_on:\n",
    "                file2_data[column] = file2_data[column].astype(merge_dtype)\n",
    "        \n",
    "        for chunk1 in pd.read_csv(file1, chunksize=chunksize):\n",
    "            if merge_dtype is not None:\n",
    "                for column in merge_on:\n",
    "                    chunk1[column] = chunk1[column].astype(merge_dtype)\n",
    "\n",
    "            merged_chunk = pd.merge(chunk1, file2_data, on=merge_on) if merge_on else pd.concat([chunk1, file2_data], axis=1)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = csv.DictWriter(f_out, fieldnames=merged_chunk.columns)\n",
    "                writer.writeheader()\n",
    "\n",
    "            for row in merged_chunk.to_dict(orient='records'):\n",
    "                writer.writerow(row)\n",
    "                \n",
    "\n",
    "                    \n",
    "# Run time test function\n",
    "def time_function(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "# Folder check and creation \n",
    "def create_folders(folder_paths):\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Folder created: {folder_path}\")\n",
    "        else:\n",
    "            print(f\"Folder already exists: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "074d654c-d1f2-41f6-b807-f9df10707c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filters\n",
    "def empty_filter_func(chunk):\n",
    "    # Empty filter for test\n",
    "    return chunk\n",
    "\n",
    "def filter_empty_to_0_and_dropna(chunk):\n",
    "    # Replace empty values with 0 if the column is numeric, otherwise with NaN\n",
    "    for col in chunk.columns:\n",
    "        if pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "            chunk[col] = chunk[col].replace('', 0).fillna(0)\n",
    "        else:\n",
    "            chunk[col] = chunk[col].replace('', np.nan)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    filtered_chunk = chunk.dropna()\n",
    "    \n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_remove_empty_and_nan(chunk):\n",
    "    filtered_chunk = chunk.replace('', np.nan)\n",
    "    # Remove rows with NaN values from the DataFrame\n",
    "    filtered_chunk = filtered_chunk.dropna()\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_header_up(chunk):\n",
    "    # Turn header to upper\n",
    "    chunk.columns = map(str.upper, chunk.columns)\n",
    "    return chunk\n",
    "\n",
    "def filter_week(chunk):\n",
    "    # The data begins from week 128 (02/20/92). To filter for the year 1993, we select week 173 to 224. Based on Dominicks Manual and Codebook - Part 8: Week’s Decode Table\n",
    "    # 173 = 12/31/92 to 01/06/93\n",
    "    # 124 = 12/23/93 to 12/29/93\n",
    "    start_week = 173\n",
    "    end_week = 224\n",
    "\n",
    "    # Create a copy of the chunk to avoid the warning\n",
    "    chunk_copy = chunk.copy()\n",
    "\n",
    "    # Modify the 'WEEK' column in the copied chunk\n",
    "    chunk_copy['WEEK'] = chunk_copy['WEEK'].astype(int)\n",
    "\n",
    "    filtered_chunk = chunk_copy[(chunk_copy['WEEK'] >= start_week) & (chunk_copy['WEEK'] <= end_week)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_compute_revenue(chunk):\n",
    "    # Compute revenue \"PRICE\" * \"MOVE\" and store it in a new column \"REVENUE\"\n",
    "    chunk['REVENUE'] = chunk['PRICE'] * chunk['MOVE']\n",
    "    return chunk\n",
    "\n",
    "def filter_move_above_one(chunk):\n",
    "    filtered_chunk = chunk[(chunk['MOVE'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_out_bad_data(chunk):\n",
    "    filtered_chunk = chunk[(chunk['OK'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_keep_columns(chunk, columns_to_keep = [\"STORE\", \"COSMETIC\", \"HABA\",\"PHARMACY\",\"WEEK\"]):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def remove_column(chunk):\n",
    "    column_names = [\"GINI\",\"LIFT5\",\"RATIO5\"]\n",
    "    for column_name in column_names:\n",
    "        if column_name in chunk.columns:\n",
    "            chunk = chunk.drop(column_name, axis=1)\n",
    "    return chunk\n",
    "\n",
    "def combine_same_week_and_store(chunk):\n",
    "    # Combine rows with the same value in the \"WEEK\" column\n",
    "    combined_chunk = chunk.groupby([\"WEEK\",\"STORE\"]).sum().reset_index()\n",
    "    return combined_chunk\n",
    "\n",
    "def filter_remove_symb_in_description(chunk):\n",
    "    # Combine rows with the same value in the \"WEEK\" column\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r'&', ' ANNNND ', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r'[^\\w]', '', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r' ANNNND ', '&', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r' +', ' ', x))\n",
    "    return chunk\n",
    "\n",
    "\"\"\"\n",
    "def filter_clean_text_data(chunk, column = \"DESCRIP\"): #Note: one varible still has double spaces (!it might not be spaces!)\n",
    "    # Convert to uppercase, remove symbols, and remove extra spaces\n",
    "    chunk[column] = (chunk[column].str.upper()\n",
    "                                  .str.replace(r\"\\W+\", \" \", regex=True)\n",
    "                                  .str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "                                  .str.strip()\n",
    "                                  .str.replace(r\"(?<!\\S)\\S(?!\\S)\", \"\", regex=True))\n",
    "    return chunk\n",
    "\"\"\"\n",
    "def filter_clean_text_data(chunk, column = \"DESCRIP\"):\n",
    "    # Convert to uppercase\n",
    "    chunk[column] = chunk[column].str.upper()\n",
    "    # Replace non breaking spaces and invisible characters\n",
    "    chunk[column] = chunk[column].apply(lambda x: ''.join(c if c.isprintable() else ' ' for c in x))\n",
    "    # Remove symbols\n",
    "    chunk[column] = chunk[column].str.replace(r\"\\W+\", \" \", regex=True)\n",
    "    # Remove single standalone characters\n",
    "    chunk[column] = chunk[column].str.replace(r\"\\b\\w\\b\", \" \", regex=True)\n",
    "\n",
    "    # Keep replacing double spaces overly complex vesion\n",
    "    while chunk[column].str.contains('  ', regex=False).any():\n",
    "        chunk[column] = chunk[column].str.replace('  ', ' ', regex=False)\n",
    "\n",
    "    # Strip extra white space from beginning and end\n",
    "    chunk[column] = chunk[column].str.strip()\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3b199f3-964e-4eb5-8f40-7280ff59ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: data/\n",
      "Folder already exists: data_temp/\n",
      "Folder already exists: data_clean/\n",
      "[['data/upcsha.csv', '.csv'], ['data/wsha.csv', '.csv'], ['data/ccount.dta', '.dta'], ['data/demo.dta', '.dta']]\n",
      "\n",
      "! Remember the raw data needs to be in the data folder !\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data/\" # folder with data\n",
    "folder_path_temp = \"data_temp/\" # folder with temp data\n",
    "folder_path_clean = \"data_clean/\" # folder with clean\n",
    "chunk_size = 10_000 # chunks of data loaderd in memory\n",
    "\n",
    "create_folders([folder_path, folder_path_temp,folder_path_clean])\n",
    "\n",
    "file_paths = get_files(folder_path,\".csv\")\n",
    "file_paths.extend(get_files(folder_path,\".dta\"))\n",
    "\n",
    "print(file_paths)\n",
    "print(\"\\n! Remember the raw data needs to be in the data folder !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dba118-51a9-45d6-9dbb-df78e6588ec8",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c89077-110e-4a92-adc7-d2d184736485",
   "metadata": {},
   "source": [
    "#### Clean upc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5d72a176-48d4-4409-abf4-a746bd635cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read the file with encoding 'utf-8', trying the next one...\n",
      "Succes with the encoding 'ISO-8859-1', file data_temp/upcsha_temp.csv now created\n",
      "load_and_filter_file took 0.75 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_clean_text_data,filter_remove_empty_and_nan]\n",
    "time_function(load_and_filter_file, file_paths[0][0], folder_path_temp, filter_func_list, file_type=file_paths[0][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8447dc10-82d8-493d-a149-adcea49496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brand Probabilety\n",
    "# Load the DataFrame from a CSV file\n",
    "file_path_ups_temp = \"data_temp/upcsha_temp.csv\"\n",
    "df_upc = pd.read_csv(file_path_ups_temp)\n",
    "\n",
    "# Split each product description into words\n",
    "df_upc['words'] = df_upc['DESCRIP'].str.split()\n",
    "\n",
    "# Get the first word as potential brand name\n",
    "df_upc['potential_brand'] = df_upc['words'].apply(lambda x: x[0] if x else '')\n",
    "\n",
    "# Initialize an empty dictionary to store final brand names\n",
    "brand_names = {}\n",
    "\n",
    "# For each unique potential brand name\n",
    "for potential_brand in df_upc['potential_brand'].unique():\n",
    "    # Get all product descriptions for this potential brand\n",
    "    descriptions = df_upc.loc[df_upc['potential_brand'] == potential_brand, 'words']\n",
    "    \n",
    "    # Count the frequency of each word in these descriptions (excluding the first word)\n",
    "    word_counts = Counter(word for desc in descriptions for word in desc[1:])\n",
    "    \n",
    "    # Get the most frequent word\n",
    "    most_common_word = word_counts.most_common(1)[0][0] if word_counts else ''\n",
    "    \n",
    "    # Combine the potential brand name and the most common word to form the final brand name\n",
    "    brand_names[potential_brand] = potential_brand + ' ' + most_common_word\n",
    "\n",
    "# Map potential brand names to final brand names in the DataFrame\n",
    "df_upc['BRAND'] = df_upc['potential_brand'].map(brand_names)\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df_upc.to_csv(file_path_ups_temp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959cc42-9b57-4c2b-81f1-cf38ee63b40b",
   "metadata": {},
   "source": [
    "#### Clean walk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0647633e-a988-4663-8137-2a1359264e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/wsha_temp.csv now created\n",
      "load_and_filter_file took 39.28 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_move_above_one,filter_out_bad_data,filter_compute_revenue]\n",
    "time_function(load_and_filter_file, file_paths[1][0], folder_path_temp, filter_func_list, file_type=file_paths[1][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b68c68-3bed-48ff-a44c-df68e37d8c40",
   "metadata": {},
   "source": [
    "#### Clean customer count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96fa756b-a7f6-4f5f-8e5c-df11d6cf01b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/ccount_temp.csv now created\n",
      "load_and_filter_file took 4.54 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_keep_columns, combine_same_week_and_store]\n",
    "time_function(load_and_filter_file, file_paths[2][0], folder_path_temp, filter_func_list, file_type=file_paths[2][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2278b-b318-4b47-a170-4d559847ed06",
   "metadata": {},
   "source": [
    "#### Clean demo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23fa9ae8-2de6-471a-b42a-06949c94c5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/demo_temp.csv now created\n",
      "load_and_filter_file took 0.83 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up, remove_column,filter_empty_to_0_and_dropna]\n",
    "time_function(load_and_filter_file, file_paths[3][0], folder_path_temp, filter_func_list, file_type=file_paths[3][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974b83f-33f3-4cd6-a714-7d8fafdea7a8",
   "metadata": {},
   "source": [
    "### Merge Data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01b2b6a4-80d2-4d06-855c-827290947e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 6.22 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_temp.csv\"\n",
    "merge_file_sec = \"data_temp/upcsha_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"UPC\", merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9a20f37c-2b26-462e-acb4-068d3cf6e8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 6.53 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_upcsha.csv\"\n",
    "merge_file_sec = \"data_temp/ccount_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha_ccount.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=[\"STORE\",\"WEEK\"], merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "548934c2-87ed-48d6-b8ed-3e408307a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 147.62 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_upcsha_ccount.csv\"\n",
    "merge_file_sec = \"data_temp/demo_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha_ccount_demo.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"STORE\", merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d42a8d-9f8d-405a-8936-3d086ef8cbf3",
   "metadata": {},
   "source": [
    "### Clean Merge Data and Select Relevant Column\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73bb0415-caf3-489d-b3d6-7f533071d250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_clean/shampoo_sale_data.csv now created\n",
      "load_and_filter_file took 15.79 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\"STORE\", \"WEEK\", \"CITY\", \"ZIP\", \"BRAND\", \"MOVE\", \"PRICE\",\"QTY\",\"REVENUE\",\"PROFIT\",\"CASE\",\"COSMETIC\",\"HABA\",\"PHARMACY\",\"INCOME\",\"HSIZEAVG\",\"HSIZE1\",\"HSIZE2\",\n",
    "                    \"HSIZE34\",\"HHLARGE\",\"SINGLE\",\"RETIRED\",\"UNEMP\",\"WORKWOM\",\"WRKCH5\",\"WRKCH17\",\"NWRKCH5\",\"NWRKCH17\",\"WRKCH\",\"NWRKCH\",\"WRKWNCH\"]\n",
    "\n",
    "def filter_select_columns(chunk, columns_to_keep = selected_columns):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_upc(chunk): #!! Note: remove, not in use any more....I think....double check !!\n",
    "    filtered_chunk = chunk[(chunk[\"UPC\"].isin(upc_list[-1]))]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_create_sales_column(chunk): #!! Note: remove, not in use any more !!\n",
    "    # Create a temporary DataFrame to avoid unkown SettingWithCopyWarning\n",
    "    # NOTE I still get the error, tho the code works. \n",
    "    # Error is most likly panda not knowing wheter I want to create a copy or work with the main chunk. \n",
    "    tmp = chunk['PRICE'] * chunk[\"MOVE\"] / chunk['QTY']\n",
    "    \n",
    "    # Assign the temporary DataFrame to the new column 'SALES'\n",
    "    chunk = chunk.assign(SALES=tmp)\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "filter_func_list = [filter_select_columns]\n",
    "file_main = \"data_temp/wsha_upcsha_ccount_demo.csv\"\n",
    "file_out = \"shampoo_sale_data.csv\"\n",
    "time_function(load_and_filter_file, file_main, folder_path_clean, filter_func_list, chunksize = chunk_size, new_file_name = file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72f752da-c82f-4dd1-98d3-633dead9c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"STORE\", \"CITY\", \"ZIP\", \"BRAND\", and \"WEEK\" and sum the other columns\n",
    "df_sale = pd.read_csv(\"data_clean/shampoo_sale_data.csv\")\n",
    "df_sale_grouped = df_sale.groupby([\"WEEK\", \"STORE\", \"CITY\", \"ZIP\", \"BRAND\"]).sum().reset_index()\n",
    "\n",
    "# Save the updated DataFrame back to the CSV file\n",
    "df_sale_grouped.to_csv(\"data_clean/shampoo_sale_data_branded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fbd86-db81-4fdd-aba1-78e1ffc8ff65",
   "metadata": {},
   "source": [
    "### Find Top Brand\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "202cb055-57c0-4f4f-8219-b85c560ecf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def read_data(csv_file):\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "def get_top_brands(df, brand_column, move_column, profit_column, n_brands=5):\n",
    "    score_column = \"move and profit score\"\n",
    "    df[score_column] = df[move_column] + df[profit_column]\n",
    "    top_brands_series = df.groupby(brand_column)[score_column].sum().nlargest(n_brands)\n",
    "    top_brands = top_brands_series.index.tolist()\n",
    "    print(\"Top brands, highest move and profit:\")\n",
    "    for brand, score in top_brands_series.items():\n",
    "        total_move = df[df[brand_column] == brand][move_column].sum()\n",
    "        total_profit = df[df[brand_column] == brand][profit_column].sum()\n",
    "        print(f\"Brand: {brand}, Total MOVE: {total_move}, Total PROFIT: {total_profit}, Score (Move + Profit): {score}\")\n",
    "    return top_brands\n",
    "\n",
    "def filter_by_brands(df, brand_column, top_brands):\n",
    "    return df[df[brand_column].isin(top_brands)]\n",
    "\n",
    "def write_data(df, csv_file):\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\"\"\"\n",
    "def read_data(csv_file):\n",
    "    return pd.read_csv(csv_file)\n",
    "\n",
    "def get_top_brands(df, brand_column, move_column, profit_column, n_brands=5):\n",
    "    score_column = \"move and profit score\"\n",
    "    df[score_column] = df[move_column] + df[profit_column]\n",
    "    top_brands_series = df.groupby(brand_column)[score_column].sum().nlargest(n_brands)\n",
    "    top_brands = top_brands_series.index.tolist()\n",
    "    print(\"Top brands, highest move and profit:\")\n",
    "    for brand, score in top_brands_series.items():\n",
    "        total_move = df[df[brand_column] == brand][move_column].sum()\n",
    "        total_profit = df[df[brand_column] == brand][profit_column].sum()\n",
    "        print(f\"Brand: {brand}, Total MOVE: {total_move}, Total PROFIT: {total_profit}, Score (Move + Profit): {score}\")\n",
    "    return top_brands\n",
    "\n",
    "def filter_by_brands(df, brand_column, top_brands, group_columns = [\"WEEK\", \"STORE\", \"CITY\", \"ZIP\", \"BRAND\"]):\n",
    "    # Change brands that are not in top_brands to 'OTHERS'\n",
    "    df.loc[~df[brand_column].isin(top_brands), brand_column] = 'OTHERS'\n",
    "    \n",
    "    # Group by the defined columns and compute the sum for each group\n",
    "    df_grouped = df.groupby(group_columns).sum().reset_index()\n",
    "    \n",
    "    return df_grouped\n",
    "\n",
    "\n",
    "def write_data(df, csv_file):\n",
    "    df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2df597a8-501c-4624-b6e7-cc4f92c0798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top brands, highest move and profit:\n",
      "Brand: RAVE AERO, Total MOVE: 80490, Total PROFIT: 438633.52, Score (Move + Profit): 519123.52\n",
      "Brand: SUAVE COND, Total MOVE: 84329, Total PROFIT: 388499.98, Score (Move + Profit): 472828.98\n",
      "Brand: WHITE RAIN, Total MOVE: 45172, Total PROFIT: 409872.29, Score (Move + Profit): 455044.29\n",
      "Brand: WHT RN, Total MOVE: 24819, Total PROFIT: 267167.47, Score (Move + Profit): 291986.47\n",
      "Brand: SALON SELECT, Total MOVE: 24263, Total PROFIT: 170093.45, Score (Move + Profit): 194356.45\n"
     ]
    }
   ],
   "source": [
    "file_main = \"data_clean/shampoo_sale_data_branded.csv\"\n",
    "file_top5 = \"data_clean/shampoo_sale_data_top5.csv\"\n",
    "brand_column = \"BRAND\"\n",
    "move_column = \"MOVE\"\n",
    "profit_column = \"PROFIT\"\n",
    "\n",
    "df = read_data(file_main)\n",
    "top_brands = top_brands = get_top_brands(df, brand_column, move_column, profit_column)\n",
    "df = filter_by_brands(df, brand_column, top_brands)\n",
    "write_data(df, file_top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522050a7-54dd-4862-a5cb-2631251d8c2b",
   "metadata": {},
   "source": [
    "### Add month column and reorder the data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c3c16ca-1deb-47a5-834c-adcfb5ff4c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved as data_clean/shampoo_sale_data_master_file.csv\n"
     ]
    }
   ],
   "source": [
    "def add_month_year(input_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = read_data(input_file)\n",
    "\n",
    "    # Function to convert week encoding to month and year\n",
    "    def week_to_month_year(week):\n",
    "        start_date = datetime.strptime(\"1992-12-31\", \"%Y-%m-%d\")\n",
    "        week_middle_date = start_date + timedelta(weeks=(week - 173), days=3)  # Get the middle of the week\n",
    "        return week_middle_date.month, week_middle_date.year\n",
    "\n",
    "    # Convert \"WEEK\" to \"MONTH\" and \"YEAR\" using the week_to_month_year function\n",
    "    df[\"MONTH\"], df[\"YEAR\"] = zip(*df[\"WEEK\"].apply(week_to_month_year))\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"The file has been processed and saved as {output_file}\")\n",
    "\n",
    "input_file = \"data_clean/shampoo_sale_data_top5.csv\"\n",
    "output_file = \"data_clean/shampoo_sale_data_master_file.csv\"\n",
    "\n",
    "add_month_year(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "910f6b4e-b0f9-409a-9377-b2ecedfa4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved as data_clean/shampoo_sale_data_master_file.csv\n"
     ]
    }
   ],
   "source": [
    "def reorder_columns(input_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = read_data(input_file)\n",
    "\n",
    "    # Specify the desired column order\n",
    "    column_order = [\"WEEK\", \"MONTH\", \"YEAR\", \"STORE\", \"CITY\", \"ZIP\", \"BRAND\", \"MOVE\", \"PRICE\", \"QTY\", \"REVENUE\", \"PROFIT\"]\n",
    "\n",
    "    # Add any additional columns in the DataFrame that aren't specified in the column order\n",
    "    column_order += [col for col in df.columns if col not in column_order]\n",
    "\n",
    "    # Reorder the DataFrame's columns\n",
    "    df = df.reindex(columns=column_order)\n",
    "    \n",
    "    # Save the reordered DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"The file has been processed and saved as {output_file}\")\n",
    "\n",
    "input_file = \"data_clean/shampoo_sale_data_master_file.csv\"\n",
    "output_file = \"data_clean/shampoo_sale_data_master_file.csv\"\n",
    "\n",
    "reorder_columns(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b69c7-703d-4646-ab9c-d5fbd5597ab2",
   "metadata": {},
   "source": [
    "### Data in the set\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e805843-ba62-48cf-865a-e3e91a2f6bab",
   "metadata": {},
   "source": [
    "**WEEK:** This denotes the week number when the sales occurred.\n",
    "\n",
    "**MONTH:** This is the month when the sales occured.\n",
    "\n",
    "**YEAR:** This is the year when the sales occured.\n",
    "\n",
    "**STORE:** This is the unique identifier for each store location.\n",
    "\n",
    "**CITY:** This is the city name for the stores location.\n",
    "\n",
    "**ZIP:** This is the zip code for the stores location.\n",
    "\n",
    "**BRAND:** This is the grouped product description\n",
    "\n",
    "**MOVE:** This is the number of units of the product that were sold.\n",
    "\n",
    "**PRICE:** This is the retail price of the product.\n",
    "\n",
    "**QTY:** This is the number of items bundled together.\n",
    "\n",
    "**CASE:** This is the number of items in a case.\n",
    "\n",
    "**COSMETIC, HABA, PHARMACY:** These variables are to be related to the different categories of products, and Sales in Dollars.\n",
    "\n",
    "**INCOME:** This is the gross income, cents on the dollar for each item sold.\n",
    "\n",
    "**HSIZEAVG, HSIZE1, HSIZE2, HSIZE34, HHLARGE:** These variables seem to represent household sizes.\n",
    "\n",
    "**SINGLE, RETIRED, UNEMP, WORKWOM, WRKCH5, WRKCH17, NWRKCH5, NWRKCH17, WRKCH, NWRKCH, WRKWNCH:** These variables represent different demographics related to employment and family status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9681ae-19e6-49b0-8c4d-83d967995ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59137930-faf1-4dcb-bd27-2097521c47e8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "##### !! NOTE !!\n",
    "To run this code you need the data from [chicagobooth](https://www.chicagobooth.edu/research/kilts/datasets/dominicks), specifically the data for shampoo.\n",
    "\n",
    "The filles needed is:\n",
    "\n",
    "**Customoer Count File** - (ccount(stata).zip)\n",
    "\n",
    "**Store-Level Demographics File** - (demo(stata).zip)\n",
    "\n",
    "And from the **category file** you need to find **shampoo**, and download **UPC.csv File** and **Movement.csv File**\n",
    "\n",
    "And all of this needs to be saved in a folder called **\"data\"** in the same folder you are running this script from, running the **\"create_folders\" function** will automatically create the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f55135f-80a1-4528-a60d-2f4f8af12826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import chardet\n",
    "import time\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import sympy as sp\n",
    "from sympy.solvers import solve\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from cycler import cycler\n",
    "\n",
    "# custome plot style\n",
    "params  = {\n",
    "\"lines.linewidth\": 1.5,\n",
    "\n",
    "\"legend.fancybox\": \"true\",\n",
    "\n",
    "\"axes.prop_cycle\": cycler('color', [\"#ffa822\",\"#1ac0c6\",\"#ff6150\",\"#30B66A\",\"#B06AFF\",\"#FF21E1\"]),\n",
    "\"axes.facecolor\": \"#2b2b2b\",\n",
    "\"axes.axisbelow\": \"true\",\n",
    "\"axes.grid\": \"true\",\n",
    "\"axes.edgecolor\": \"#2b2b2b\",\n",
    "\"axes.linewidth\": 0.5,\n",
    "\"axes.labelpad\": 0,\n",
    "\n",
    "\"patch.edgecolor\": \"#2b2b2b\",\n",
    "\"patch.linewidth\": 0.5,\n",
    "\n",
    "\"grid.linestyle\": \"--\",\n",
    "\"grid.linewidth\": 0.5,\n",
    "\"grid.color\": \"#b8aba7\",\n",
    "\n",
    "\"xtick.major.size\": 0,\n",
    "\"xtick.minor.size\": 0,\n",
    "\"ytick.major.size\": 0,\n",
    "\"ytick.minor.size\": 0,\n",
    "\n",
    "\"font.family\":\"monospace\",\n",
    "\"font.size\":10.0,\n",
    "\"text.color\": \"#FFE9E3\",\n",
    "\"axes.labelcolor\": \"#b8aba7\",\n",
    "\"xtick.color\": \"#b8aba7\",\n",
    "\"ytick.color\": \"#b8aba7\",\n",
    "\n",
    "\"savefig.edgecolor\": \"#2b2b2b\",\n",
    "\"savefig.facecolor\": \"#2b2b2b\",\n",
    "\n",
    "\"figure.subplot.left\": 0.08,\n",
    "\"figure.subplot.right\": 0.95,\n",
    "\"figure.subplot.bottom\": 0.09,\n",
    "\"figure.facecolor\": \"#2b2b2b\"}\n",
    "\n",
    "pylab.rcParams.update(params)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77924d09-c8b0-45c5-8c33-43e1a565b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect files from folder, if file type equals file_type\n",
    "def get_files(folder, file_type):\n",
    "    file_paths = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(f\"{file_type}\"):\n",
    "            file_paths.append([os.path.join(folder, file), file_type])\n",
    "    return file_paths\n",
    "\n",
    "def get_encoder(file_path, chunksize = 10_000):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(chunksize))\n",
    "    return result['encoding']\n",
    "\n",
    "# NOTE TO SELF, STRAGE ERROR HERE, MIGHT BE A DUPLICATE ERROR\n",
    "# Load, filter in chunks and Convert to csv\n",
    "def load_and_filter_file(input_file, temp_path, filter_func:list, file_type=\".csv\", chunksize=10_000, new_file_name = \"\"):\n",
    "    temp_file = os.path.join(temp_path,new_file_name)\n",
    "    if new_file_name == \"\":\n",
    "        # Extract the file name from the input_file path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Create a temp_file path by combining temp_path and input_file_name\n",
    "        file_name_without_ext, file_ext = os.path.splitext(input_file_name)\n",
    "        temp_file = os.path.join(temp_path, f\"{file_name_without_ext}_temp.csv\")\n",
    "        \n",
    "    # Had to fix the decoding because 'invalid continuation byte' that utf-8 can't decode. And manual attempt to fix it did not reveal byte 0xd5\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"cp1252\", \"latin1\"]\n",
    "    success = False\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            if file_type.lower() == '.csv':\n",
    "                reader = pd.read_csv(input_file, chunksize=chunksize, encoding=encoding)\n",
    "            elif file_type.lower() == '.dta':\n",
    "                reader = pd.read_stata(input_file, chunksize=chunksize)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'dta'.\")\n",
    "\n",
    "            for i, chunk in enumerate(reader):\n",
    "                filtered_chunk = chunk\n",
    "                for func in filter_func:\n",
    "                    filtered_chunk = func(filtered_chunk)\n",
    "                if i == 0:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='w')\n",
    "                else:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='a', header=False)\n",
    "\n",
    "            success = True\n",
    "            print(f\"Succes with the encoding '{encoding}', file {temp_file} now created\")\n",
    "            break\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read the file with encoding '{encoding}', trying the next one...\")\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"None of the attempted encodings were successful in reading the file.\")\n",
    "            \n",
    "# Merge csv files\n",
    "\"\"\"\n",
    "def merge_csv_files(file1, file2, output_file, merge_on=None, merge_dtype=None, chunksize=10000):\n",
    "    if (merge_on is not None) and (type(merge_on) != list):  # Fix the condition here\n",
    "        merge_on = [merge_on]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = None\n",
    "        for chunk1 in pd.read_csv(file1, chunksize=chunksize):\n",
    "            for chunk2 in pd.read_csv(file2, chunksize=chunksize):\n",
    "                if merge_dtype is not None:\n",
    "                    for column in merge_on:\n",
    "                        chunk1[column] = chunk1[column].astype(merge_dtype)\n",
    "                        chunk2[column] = chunk2[column].astype(merge_dtype)\n",
    "\n",
    "                merged_chunk = pd.merge(chunk1, chunk2, on=merge_on) if merge_on else pd.concat([chunk1, chunk2], axis=1)\n",
    "\n",
    "                if writer is None:\n",
    "                    writer = csv.DictWriter(f_out, fieldnames=merged_chunk.columns)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                for row in merged_chunk.to_dict(orient='records'):\n",
    "                    writer.writerow(row)\n",
    "\"\"\"\n",
    "\n",
    "def merge_csv_files(file1, file2, output_file, merge_on= None, merge_dtype=None, chunksize =10000):\n",
    "    if (merge_on is not None) and (type(merge_on) != list):  # Fix the condition here\n",
    "        merge_on = [merge_on]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = None\n",
    "        \n",
    "        # Read the entire file2 into memory\n",
    "        file2_data = pd.read_csv(file2)\n",
    "        \n",
    "        if merge_dtype is not None:\n",
    "            for column in merge_on:\n",
    "                file2_data[column] = file2_data[column].astype(merge_dtype)\n",
    "        \n",
    "        for chunk1 in pd.read_csv(file1, chunksize=chunksize):\n",
    "            if merge_dtype is not None:\n",
    "                for column in merge_on:\n",
    "                    chunk1[column] = chunk1[column].astype(merge_dtype)\n",
    "\n",
    "            merged_chunk = pd.merge(chunk1, file2_data, on=merge_on) if merge_on else pd.concat([chunk1, file2_data], axis=1)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = csv.DictWriter(f_out, fieldnames=merged_chunk.columns)\n",
    "                writer.writeheader()\n",
    "\n",
    "            for row in merged_chunk.to_dict(orient='records'):\n",
    "                writer.writerow(row)\n",
    "                \n",
    "\n",
    "                    \n",
    "# Run time test function\n",
    "def time_function(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "# Folder check and creation \n",
    "def create_folders(folder_paths):\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Folder created: {folder_path}\")\n",
    "        else:\n",
    "            print(f\"Folder already exists: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c241d4c-38a2-4533-bd84-a1e6bd4407f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filters\n",
    "def empty_filter_func(chunk):\n",
    "    # Empty filter for test\n",
    "    return chunk\n",
    "\n",
    "def filter_empty_to_0_and_dropna(chunk):\n",
    "    # Replace empty values with 0 if the column is numeric, otherwise with NaN\n",
    "    for col in chunk.columns:\n",
    "        if pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "            chunk[col] = chunk[col].replace('', 0).fillna(0)\n",
    "        else:\n",
    "            chunk[col] = chunk[col].replace('', np.nan)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    filtered_chunk = chunk.dropna()\n",
    "    \n",
    "    return filtered_chunk\n",
    "\n",
    "\n",
    "def filter_remove_empty_and_nan(chunk):\n",
    "    filtered_chunk = chunk.replace('', np.nan)\n",
    "    # Remove rows with NaN values from the DataFrame\n",
    "    filtered_chunk = filtered_chunk.dropna()\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_header_up(chunk):\n",
    "    # Turn header to upper\n",
    "    chunk.columns = map(str.upper, chunk.columns)\n",
    "    return chunk\n",
    "\n",
    "def filter_week(chunk):\n",
    "    # The data begins from week 128 (02/20/92). To filter for the year 1993, we select week 173 to 225.\n",
    "    start_week = 173\n",
    "    end_week = 225\n",
    "\n",
    "    # Create a copy of the chunk to avoid the warning\n",
    "    chunk_copy = chunk.copy()\n",
    "\n",
    "    # Modify the 'WEEK' column in the copied chunk\n",
    "    chunk_copy['WEEK'] = chunk_copy['WEEK'].astype(int)\n",
    "\n",
    "    filtered_chunk = chunk_copy[(chunk_copy['WEEK'] >= start_week) & (chunk_copy['WEEK'] <= end_week)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_move_above_one(chunk):\n",
    "    filtered_chunk = chunk[(chunk['MOVE'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_out_bad_data(chunk):\n",
    "    filtered_chunk = chunk[(chunk['OK'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_keep_columns(chunk, columns_to_keep = [\"STORE\", \"COSMETIC\", \"HABA\",\"PHARMACY\",\"WEEK\"]):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def remove_column(chunk):\n",
    "    column_names = [\"GINI\",\"LIFT5\",\"RATIO5\"]\n",
    "    for column_name in column_names:\n",
    "        if column_name in chunk.columns:\n",
    "            chunk = chunk.drop(column_name, axis=1)\n",
    "    return chunk\n",
    "\n",
    "def combine_same_week_and_store(chunk):\n",
    "    # Combine rows with the same value in the \"WEEK\" column\n",
    "    combined_chunk = chunk.groupby([\"WEEK\",\"STORE\"]).sum().reset_index()\n",
    "    return combined_chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9c894-500a-4484-b46c-2e2470a59a37",
   "metadata": {},
   "source": [
    "### Filter and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f097ae5b-d230-4852-a36e-372ddf19fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: data_temp/\n",
      "Folder already exists: data_clean/\n",
      "[['data/upcsha.csv', '.csv'], ['data/wsha.csv', '.csv'], ['data/ccount.dta', '.dta'], ['data/demo.dta', '.dta']]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data/\" # folder with data\n",
    "folder_path_temp = \"data_temp/\" # folder with temp data\n",
    "folder_path_clean = \"data_clean/\" # folder with clean\n",
    "chunk_size = 10_000 # chunks of data loaderd in memory\n",
    "\n",
    "create_folders([folder_path_temp,folder_path_clean])\n",
    "\n",
    "file_paths = get_files(folder_path,\".csv\")\n",
    "file_paths.extend(get_files(folder_path,\".dta\"))\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d3987-58e2-4d66-9384-63a705431803",
   "metadata": {},
   "source": [
    "#### upc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5156a4-1a2d-4371-bf37-e0f59e363cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read the file with encoding 'utf-8', trying the next one...\n",
      "Succes with the encoding 'ISO-8859-1', file data_temp/upcsha_temp.csv now created\n",
      "load_and_filter_file took 0.02 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan]\n",
    "time_function(load_and_filter_file, file_paths[0][0], folder_path_temp, filter_func_list, file_type=file_paths[0][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37520239-274f-47ef-aad8-b9b626af5412",
   "metadata": {},
   "source": [
    "#### walk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "735c9c81-1a51-41a6-bb5d-28875eee7d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/wsha_temp.csv now created\n",
      "load_and_filter_file took 31.21 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_move_above_one,filter_out_bad_data]\n",
    "time_function(load_and_filter_file, file_paths[1][0], folder_path_temp, filter_func_list, file_type=file_paths[1][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67974c31-1864-4c54-b60f-fc5055baaaa7",
   "metadata": {},
   "source": [
    "#### custumer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae5a6e7-2d31-414f-9bcc-af0f45628a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/ccount_temp.csv now created\n",
      "load_and_filter_file took 1.02 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_keep_columns, combine_same_week_and_store]\n",
    "time_function(load_and_filter_file, file_paths[2][0], folder_path_temp, filter_func_list, file_type=file_paths[2][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144f839-6ec4-415e-baee-802240da17c6",
   "metadata": {},
   "source": [
    "#### demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45756e4-2fe4-4be7-a8fd-f6350570821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/demo_temp.csv now created\n",
      "load_and_filter_file took 0.34 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up, remove_column,filter_empty_to_0_and_dropna]\n",
    "time_function(load_and_filter_file, file_paths[3][0], folder_path_temp, filter_func_list, file_type=file_paths[3][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c54b19-14e6-4927-ab6c-b5ba92252a9d",
   "metadata": {},
   "source": [
    "### Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1238710-c2c6-4fc3-957e-a9be4acec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 3.50 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_temp.csv\"\n",
    "merge_file_sec = \"data_temp/upcsha_temp.csv\"\n",
    "merge_file_out = \"data_clean/wsha_upcsha.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"UPC\", merge_dtype=int, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a41e333b-c62d-42af-8098-df15a0295d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 4.32 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_clean/wsha_upcsha.csv\"\n",
    "merge_file_sec = \"data_temp/ccount_temp.csv\"\n",
    "merge_file_out = \"data_clean/wsha_upcsha_ccount.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=[\"STORE\",\"WEEK\"], merge_dtype=int, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "821b42b4-2fc5-4962-b221-4e067397ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 107.64 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_clean/wsha_upcsha_ccount.csv\"\n",
    "merge_file_sec = \"data_temp/demo_temp.csv\"\n",
    "merge_file_out = \"data_clean/wsha_upcsha_ccount_demo.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"STORE\", merge_dtype=int, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebe46b-732b-4aae-a460-5f59e1a568ed",
   "metadata": {},
   "source": [
    "### Select Relevant Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "457257bc-6b64-4301-b190-6b9a9068bb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[96, 'WHITE RAIN SHMP RG B', 445092873], [80, '#WHT RAIN PLUS X/BOD', 445098233], [77, 'WHITE RAIN SHAMP DRY', 445092945], [69, '*PERT PLUS NORM', -594967207], [69, 'WHITE RAIN COND CLAR', 445092927]]\n"
     ]
    }
   ],
   "source": [
    "def top_n_values_with_names(input_file, column_number, column_name, column_desc, n=5):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Sort the DataFrame based on the values in the specified column_number\n",
    "    sorted_df = df.sort_values(by=column_number, ascending=False)\n",
    "\n",
    "    # Get the top N rows from the sorted DataFrame\n",
    "    top_n_rows = sorted_df.head(n)\n",
    "\n",
    "    # Create a list with the top N values and their corresponding names from column_name\n",
    "    result = top_n_rows[[column_number, column_name, column_desc]].values.tolist()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "input_file = 'data_clean/wsha_upcsha.csv'\n",
    "column_number = 'MOVE'\n",
    "column_name = 'UPC'\n",
    "column_desc = \"DESCRIP\"\n",
    "n = 5\n",
    "top_n_values = top_n_values_with_names(input_file, column_number, column_desc,column_name, n)\n",
    "print(top_n_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db823aee-1205-4071-995b-84ccc672c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_clean/shampoo_sale_data.csv now created\n",
      "load_and_filter_file took 12.00 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\"UPC\",\"STORE\", \"WEEK\",\"MOVE\",\"PRICE\",\"QTY\",\"PROFIT\",\"SALE\",\"DESCRIP\",\"CASE\",\"COSMETIC\",\"HABA\",\"PHARMACY\",\"INCOME\",\"HSIZEAVG\",\"HSIZE1\",\"HSIZE2\",\"HSIZE34\",\"HHLARGE\",\"SINGLE\",\"RETIRED\",\"UNEMP\",\"WORKWOM\",\"WRKCH5\",\"WRKCH17\",\"NWRKCH5\",\"NWRKCH17\",\"WRKCH\",\"NWRKCH\",\"WRKWNCH\"]\n",
    "\n",
    "def filter_select_columns(chunk, columns_to_keep = selected_columns):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_upc(chunk):\n",
    "    filtered_chunk = chunk[(chunk['UPC'].isin([445092873,445098233,445092945,-594967207,445092927]))]\n",
    "    return filtered_chunk\n",
    "\n",
    "filter_func_list = [filter_select_columns,filter_upc]\n",
    "file_main = \"data_clean/wsha_upcsha_ccount_demo.csv\"\n",
    "file_out = \"shampoo_sale_data.csv\"\n",
    "time_function(load_and_filter_file, file_main, folder_path_clean, filter_func_list, chunksize = chunk_size, new_file_name = file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c81238-bc1d-494a-9945-ceb565293304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

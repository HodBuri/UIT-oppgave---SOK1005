{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0691767d-3cf9-48f4-8065-caf6f8e83977",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "[Assignment Notes](https://docs.google.com/document/d/1vr6wHlQXA5iP5X3RzxpU3T7PHGUkQzULitxvV83Pf7Q/edit)\n",
    "\n",
    "[Assignment Data Notes](https://docs.google.com/document/d/1DtMgMeEk8tcsI1nWqR_ika_bmDg1ADpy64as7zDqqHc/edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59137930-faf1-4dcb-bd27-2097521c47e8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "----\n",
    "##### !! NOTE !!\n",
    "To run this code you need the data from [chicagobooth](https://www.chicagobooth.edu/research/kilts/datasets/dominicks), specifically the data for shampoo.\n",
    "\n",
    "The filles needed is:\n",
    "\n",
    "**Customoer Count File** - (ccount(stata).zip)\n",
    "\n",
    "**Store-Level Demographics File** - (demo(stata).zip)\n",
    "\n",
    "And from the **category file** you need to find **shampoo**, and download **UPC.csv File** and **Movement.csv File**\n",
    "\n",
    "And all of this needs to be saved in a folder called **\"data\"** in the same folder you are running this script from, running the **\"create_folders\" function** will automatically create the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f55135f-80a1-4528-a60d-2f4f8af12826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import chardet\n",
    "import time\n",
    "import csv\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from functools import reduce\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "import sympy as sp\n",
    "from sympy.solvers import solve\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from cycler import cycler\n",
    "\n",
    "# custome plot style\n",
    "params  = {\n",
    "\"lines.linewidth\": 1.5,\n",
    "\n",
    "\"legend.fancybox\": \"true\",\n",
    "\n",
    "\"axes.prop_cycle\": cycler('color', [\"#ffa822\",\"#1ac0c6\",\"#ff6150\",\"#30B66A\",\"#B06AFF\",\"#FF21E1\"]),\n",
    "\"axes.facecolor\": \"#2b2b2b\",\n",
    "\"axes.axisbelow\": \"true\",\n",
    "\"axes.grid\": \"true\",\n",
    "\"axes.edgecolor\": \"#2b2b2b\",\n",
    "\"axes.linewidth\": 0.5,\n",
    "\"axes.labelpad\": 0,\n",
    "\n",
    "\"patch.edgecolor\": \"#2b2b2b\",\n",
    "\"patch.linewidth\": 0.5,\n",
    "\n",
    "\"grid.linestyle\": \"--\",\n",
    "\"grid.linewidth\": 0.5,\n",
    "\"grid.color\": \"#b8aba7\",\n",
    "\n",
    "\"xtick.major.size\": 0,\n",
    "\"xtick.minor.size\": 0,\n",
    "\"ytick.major.size\": 0,\n",
    "\"ytick.minor.size\": 0,\n",
    "\n",
    "\"font.family\":\"monospace\",\n",
    "\"font.size\":10.0,\n",
    "\"text.color\": \"#FFE9E3\",\n",
    "\"axes.labelcolor\": \"#b8aba7\",\n",
    "\"xtick.color\": \"#b8aba7\",\n",
    "\"ytick.color\": \"#b8aba7\",\n",
    "\n",
    "\"savefig.edgecolor\": \"#2b2b2b\",\n",
    "\"savefig.facecolor\": \"#2b2b2b\",\n",
    "\n",
    "\"figure.subplot.left\": 0.08,\n",
    "\"figure.subplot.right\": 0.95,\n",
    "\"figure.subplot.bottom\": 0.09,\n",
    "\"figure.facecolor\": \"#2b2b2b\"}\n",
    "\n",
    "pylab.rcParams.update(params)\n",
    "print(\"finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77924d09-c8b0-45c5-8c33-43e1a565b258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect files from folder, if file type equals file_type\n",
    "def get_files(folder, file_type):\n",
    "    file_paths = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(file_type):\n",
    "            file_paths.append([os.path.join(folder, file), file_type])\n",
    "    return file_paths\n",
    "\n",
    "def get_encoder(file_path, chunksize = 10_000):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read(chunksize))\n",
    "    return result['encoding']\n",
    "\n",
    "# NOTE TO SELF, STRAGE ERROR HERE, MIGHT BE A DUPLICATE ERROR\n",
    "# Load, filter in chunks and Convert to csv\n",
    "def load_and_filter_file(input_file, temp_path, filter_func:list, file_type=\".csv\", chunksize=10_000, new_file_name = \"\"):\n",
    "    temp_file = os.path.join(temp_path,new_file_name)\n",
    "    if new_file_name == \"\":\n",
    "        # Extract the file name from the input_file path\n",
    "        input_file_name = os.path.basename(input_file)\n",
    "\n",
    "        # Create a temp_file path by combining temp_path and input_file_name\n",
    "        file_name_without_ext, file_ext = os.path.splitext(input_file_name)\n",
    "        temp_file = os.path.join(temp_path, f\"{file_name_without_ext}_temp.csv\")\n",
    "        \n",
    "    # Had to fix the decoding because 'invalid continuation byte' that utf-8 can't decode. And manual attempt to fix it did not reveal byte 0xd5\n",
    "    encodings = [\"utf-8\", \"ISO-8859-1\", \"cp1252\", \"latin1\"]\n",
    "    success = False\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            if file_type.lower() == '.csv':\n",
    "                reader = pd.read_csv(input_file, chunksize=chunksize, encoding=encoding)\n",
    "            elif file_type.lower() == '.dta':\n",
    "                reader = pd.read_stata(input_file, chunksize=chunksize)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported file type. Supported types are 'csv' and 'dta'.\")\n",
    "\n",
    "            for i, chunk in enumerate(reader):\n",
    "                filtered_chunk = chunk\n",
    "                for func in filter_func:\n",
    "                    filtered_chunk = func(filtered_chunk)\n",
    "                if i == 0:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='w')\n",
    "                else:\n",
    "                    filtered_chunk.to_csv(temp_file, index=False, mode='a', header=False)\n",
    "\n",
    "            success = True\n",
    "            print(f\"Succes with the encoding '{encoding}', file {temp_file} now created\")\n",
    "            break\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to read the file with encoding '{encoding}', trying the next one...\")\n",
    "\n",
    "    if not success:\n",
    "        raise ValueError(\"None of the attempted encodings were successful in reading the file.\")\n",
    "            \n",
    "# Merge csv files\n",
    "\"\"\"\n",
    "def merge_csv_files(file1, file2, output_file, merge_on=None, merge_dtype=None, chunksize=10000):\n",
    "    if (merge_on is not None) and (type(merge_on) != list):  # Fix the condition here\n",
    "        merge_on = [merge_on]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = None\n",
    "        for chunk1 in pd.read_csv(file1, chunksize=chunksize):\n",
    "            for chunk2 in pd.read_csv(file2, chunksize=chunksize):\n",
    "                if merge_dtype is not None:\n",
    "                    for column in merge_on:\n",
    "                        chunk1[column] = chunk1[column].astype(merge_dtype)\n",
    "                        chunk2[column] = chunk2[column].astype(merge_dtype)\n",
    "\n",
    "                merged_chunk = pd.merge(chunk1, chunk2, on=merge_on) if merge_on else pd.concat([chunk1, chunk2], axis=1)\n",
    "\n",
    "                if writer is None:\n",
    "                    writer = csv.DictWriter(f_out, fieldnames=merged_chunk.columns)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                for row in merged_chunk.to_dict(orient='records'):\n",
    "                    writer.writerow(row)\n",
    "\"\"\"\n",
    "\n",
    "def merge_csv_files(file1, file2, output_file, merge_on= None, merge_dtype=None, chunksize =10000):\n",
    "    if (merge_on is not None) and (type(merge_on) != list):  # Fix the condition here\n",
    "        merge_on = [merge_on]\n",
    "    \n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n",
    "        writer = None\n",
    "        \n",
    "        # Read the entire file2 into memory\n",
    "        file2_data = pd.read_csv(file2)\n",
    "        \n",
    "        if merge_dtype is not None:\n",
    "            for column in merge_on:\n",
    "                file2_data[column] = file2_data[column].astype(merge_dtype)\n",
    "        \n",
    "        for chunk1 in pd.read_csv(file1, chunksize=chunksize):\n",
    "            if merge_dtype is not None:\n",
    "                for column in merge_on:\n",
    "                    chunk1[column] = chunk1[column].astype(merge_dtype)\n",
    "\n",
    "            merged_chunk = pd.merge(chunk1, file2_data, on=merge_on) if merge_on else pd.concat([chunk1, file2_data], axis=1)\n",
    "\n",
    "            if writer is None:\n",
    "                writer = csv.DictWriter(f_out, fieldnames=merged_chunk.columns)\n",
    "                writer.writeheader()\n",
    "\n",
    "            for row in merged_chunk.to_dict(orient='records'):\n",
    "                writer.writerow(row)\n",
    "                \n",
    "\n",
    "                    \n",
    "# Run time test function\n",
    "def time_function(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"{func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "# Folder check and creation \n",
    "def create_folders(folder_paths):\n",
    "    for folder_path in folder_paths:\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Folder created: {folder_path}\")\n",
    "        else:\n",
    "            print(f\"Folder already exists: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c241d4c-38a2-4533-bd84-a1e6bd4407f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of filters\n",
    "def empty_filter_func(chunk):\n",
    "    # Empty filter for test\n",
    "    return chunk\n",
    "\n",
    "def filter_empty_to_0_and_dropna(chunk):\n",
    "    # Replace empty values with 0 if the column is numeric, otherwise with NaN\n",
    "    for col in chunk.columns:\n",
    "        if pd.api.types.is_numeric_dtype(chunk[col]):\n",
    "            chunk[col] = chunk[col].replace('', 0).fillna(0)\n",
    "        else:\n",
    "            chunk[col] = chunk[col].replace('', np.nan)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    filtered_chunk = chunk.dropna()\n",
    "    \n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_remove_empty_and_nan(chunk):\n",
    "    filtered_chunk = chunk.replace('', np.nan)\n",
    "    # Remove rows with NaN values from the DataFrame\n",
    "    filtered_chunk = filtered_chunk.dropna()\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_header_up(chunk):\n",
    "    # Turn header to upper\n",
    "    chunk.columns = map(str.upper, chunk.columns)\n",
    "    return chunk\n",
    "\n",
    "def filter_week(chunk):\n",
    "    # The data begins from week 128 (02/20/92). To filter for the year 1993, we select week 173 to 224. Based on Dominicks Manual and Codebook - Part 8: Week’s Decode Table\n",
    "    # 173 = 12/31/92 to 01/06/93\n",
    "    # 124 = 12/23/93 to 12/29/93\n",
    "    start_week = 173\n",
    "    end_week = 224\n",
    "\n",
    "    # Create a copy of the chunk to avoid the warning\n",
    "    chunk_copy = chunk.copy()\n",
    "\n",
    "    # Modify the 'WEEK' column in the copied chunk\n",
    "    chunk_copy['WEEK'] = chunk_copy['WEEK'].astype(int)\n",
    "\n",
    "    filtered_chunk = chunk_copy[(chunk_copy['WEEK'] >= start_week) & (chunk_copy['WEEK'] <= end_week)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_move_above_one(chunk):\n",
    "    filtered_chunk = chunk[(chunk['MOVE'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_out_bad_data(chunk):\n",
    "    filtered_chunk = chunk[(chunk['OK'] > 0)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_keep_columns(chunk, columns_to_keep = [\"STORE\", \"COSMETIC\", \"HABA\",\"PHARMACY\",\"WEEK\"]):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def remove_column(chunk):\n",
    "    column_names = [\"GINI\",\"LIFT5\",\"RATIO5\"]\n",
    "    for column_name in column_names:\n",
    "        if column_name in chunk.columns:\n",
    "            chunk = chunk.drop(column_name, axis=1)\n",
    "    return chunk\n",
    "\n",
    "def combine_same_week_and_store(chunk):\n",
    "    # Combine rows with the same value in the \"WEEK\" column\n",
    "    combined_chunk = chunk.groupby([\"WEEK\",\"STORE\"]).sum().reset_index()\n",
    "    return combined_chunk\n",
    "\n",
    "def filter_remove_symb_in_description(chunk):\n",
    "    # Combine rows with the same value in the \"WEEK\" column\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r'&', ' ANNNND ', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r'[^\\w]', '', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r' ANNNND ', '&', x))\n",
    "    chunk[\"DESCRIP\"] = chunk[\"DESCRIP\"].apply(lambda x: re.sub(r' +', ' ', x))\n",
    "    return chunk\n",
    "\n",
    "def filter_clean_text_data(chunk, column = \"DESCRIP\"):\n",
    "    # Convert to uppercase, remove symbols, and remove extra spaces\n",
    "    chunk[column] = (chunk[column].str.upper()\n",
    "                                  .str.replace(r\"\\W+\", \" \", regex=True)\n",
    "                                  .str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "                                  .str.strip()\n",
    "                                  .str.replace(r\"(?<!\\S)\\S(?!\\S)\", \"\", regex=True))\n",
    "    return chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c9c894-500a-4484-b46c-2e2470a59a37",
   "metadata": {},
   "source": [
    "### Filter and Clean Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f097ae5b-d230-4852-a36e-372ddf19fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists: data/\n",
      "Folder already exists: data_temp/\n",
      "Folder already exists: data_clean/\n",
      "[['data/upcsha.csv', '.csv'], ['data/wsha.csv', '.csv'], ['data/ccount.dta', '.dta'], ['data/demo.dta', '.dta']]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data/\" # folder with data\n",
    "folder_path_temp = \"data_temp/\" # folder with temp data\n",
    "folder_path_clean = \"data_clean/\" # folder with clean\n",
    "chunk_size = 10_000 # chunks of data loaderd in memory\n",
    "\n",
    "create_folders([folder_path, folder_path_temp,folder_path_clean])\n",
    "\n",
    "file_paths = get_files(folder_path,\".csv\")\n",
    "file_paths.extend(get_files(folder_path,\".dta\"))\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d3987-58e2-4d66-9384-63a705431803",
   "metadata": {},
   "source": [
    "#### upc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5156a4-1a2d-4371-bf37-e0f59e363cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read the file with encoding 'utf-8', trying the next one...\n",
      "Succes with the encoding 'ISO-8859-1', file data_temp/upcsha_temp.csv now created\n",
      "load_and_filter_file took 0.06 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_clean_text_data]\n",
    "time_function(load_and_filter_file, file_paths[0][0], folder_path_temp, filter_func_list, file_type=file_paths[0][1], chunksize = chunk_size)\n",
    "\n",
    "# Can add and clean description and add brands here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37520239-274f-47ef-aad8-b9b626af5412",
   "metadata": {},
   "source": [
    "#### walk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735c9c81-1a51-41a6-bb5d-28875eee7d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/wsha_temp.csv now created\n",
      "load_and_filter_file took 38.84 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_move_above_one,filter_out_bad_data]\n",
    "time_function(load_and_filter_file, file_paths[1][0], folder_path_temp, filter_func_list, file_type=file_paths[1][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67974c31-1864-4c54-b60f-fc5055baaaa7",
   "metadata": {},
   "source": [
    "#### custumer count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae5a6e7-2d31-414f-9bcc-af0f45628a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/ccount_temp.csv now created\n",
      "load_and_filter_file took 1.74 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up,filter_remove_empty_and_nan,filter_week,filter_keep_columns, combine_same_week_and_store]\n",
    "time_function(load_and_filter_file, file_paths[2][0], folder_path_temp, filter_func_list, file_type=file_paths[2][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5144f839-6ec4-415e-baee-802240da17c6",
   "metadata": {},
   "source": [
    "#### demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45756e4-2fe4-4be7-a8fd-f6350570821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_temp/demo_temp.csv now created\n",
      "load_and_filter_file took 0.41 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "filter_func_list = [filter_header_up, remove_column,filter_empty_to_0_and_dropna]\n",
    "time_function(load_and_filter_file, file_paths[3][0], folder_path_temp, filter_func_list, file_type=file_paths[3][1], chunksize = chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c54b19-14e6-4927-ab6c-b5ba92252a9d",
   "metadata": {},
   "source": [
    "### Merge Data\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1238710-c2c6-4fc3-957e-a9be4acec018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 3.86 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_temp.csv\"\n",
    "merge_file_sec = \"data_temp/upcsha_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"UPC\", merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41e333b-c62d-42af-8098-df15a0295d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 4.65 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_upcsha.csv\"\n",
    "merge_file_sec = \"data_temp/ccount_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha_ccount.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=[\"STORE\",\"WEEK\"], merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821b42b4-2fc5-4962-b221-4e067397ed13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 116.83 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "merge_file_main = \"data_temp/wsha_upcsha_ccount.csv\"\n",
    "merge_file_sec = \"data_temp/demo_temp.csv\"\n",
    "merge_file_out = \"data_temp/wsha_upcsha_ccount_demo.csv\"\n",
    "time_function(merge_csv_files,merge_file_main, merge_file_sec, merge_file_out, merge_on=\"STORE\", merge_dtype=np.int64, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcb03cc-8ec5-4bb1-9386-a06189893d84",
   "metadata": {},
   "source": [
    "### Clean Merge Data and Select Relevant Column\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db823aee-1205-4071-995b-84ccc672c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n",
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_clean/shampoo_sale_data.csv now created\n",
      "load_and_filter_file took 13.96 seconds to run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_23988\\2844662539.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  chunk['SALES'] = chunk['PRICE'] * chunk['MOVE'] / chunk['QTY']\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\"STORE\", \"CITY\", \"WEEK\",\"MOVE\",\"PRICE\",\"QTY\",\"PROFIT\",\"DESCRIP\",\"CASE\",\"COSMETIC\",\"HABA\",\"PHARMACY\",\"INCOME\",\"HSIZEAVG\",\"HSIZE1\",\"HSIZE2\",\n",
    "                    \"HSIZE34\",\"HHLARGE\",\"SINGLE\",\"RETIRED\",\"UNEMP\",\"WORKWOM\",\"WRKCH5\",\"WRKCH17\",\"NWRKCH5\",\"NWRKCH17\",\"WRKCH\",\"NWRKCH\",\"WRKWNCH\"]\n",
    "\n",
    "def filter_select_columns(chunk, columns_to_keep = selected_columns):\n",
    "    # Keep only the specified columns in the DataFrame\n",
    "    filtered_chunk = chunk[columns_to_keep]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_upc(chunk):\n",
    "    filtered_chunk = chunk[(chunk[\"UPC\"].isin(upc_list[-1]))]\n",
    "    return filtered_chunk\n",
    "\n",
    "def create_sales_column(chunk):\n",
    "    # Create a temporary DataFrame to avoid unkown SettingWithCopyWarning\n",
    "    # NOTE I still get the error, tho the code works. \n",
    "    # Error is most likly panda not knowing wheter I want to create a copy or work with the main chunk. \n",
    "    tmp = chunk['PRICE'] * chunk[\"MOVE\"] / chunk['QTY']\n",
    "    \n",
    "    # Assign the temporary DataFrame to the new column 'SALES'\n",
    "    chunk = chunk.assign(SALES=tmp)\n",
    "\n",
    "    return chunk\n",
    "\n",
    "\n",
    "filter_func_list = [filter_select_columns,filter_create_sales_column]\n",
    "file_main = \"data_temp/wsha_upcsha_ccount_demo.csv\"\n",
    "file_out = \"shampoo_sale_data.csv\"\n",
    "time_function(load_and_filter_file, file_main, folder_path_clean, filter_func_list, chunksize = chunk_size, new_file_name = file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9781775d-e943-4e48-8231-feaf81166cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_data took 7.41 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "def group_data(input_file, column, chunksize=10_000):\n",
    "    groups = {}\n",
    "    # Read the file in chunks\n",
    "    for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "        # Iterate over the cleaned data\n",
    "        for index, row in chunk.iterrows():\n",
    "            # Check if the description is missing\n",
    "            if pd.isna(row[column]):\n",
    "                continue\n",
    "                \n",
    "            # Split the description into words\n",
    "            words = row[column].split()\n",
    "\n",
    "            # Define the key as the first word or a group of similar words\n",
    "            key = words[0]\n",
    "\n",
    "            # Add the description to the corresponding group\n",
    "            if key not in groups:\n",
    "                groups[key] = []\n",
    "            groups[key].append(row[column])\n",
    "\n",
    "    return groups\n",
    "\n",
    "keyword_brand_grouping = time_function(group_data,\"data_clean/shampoo_sale_data.csv\", \"DESCRIP\", chunksize=chunk_size)\n",
    "#display(keyword_brand_grouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff89af30-6c75-4199-a894-d2a53a075de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved as data_clean/shampoo_sale_data_brand.csv\n",
      "group_by_keyword took 1.68 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "def group_by_keyword(input_file, output_file, column, keyword_dict, group_columns=[\"WEEK\", \"STORE\"], chunksize=10_000):\n",
    "    # Define a function to map each description to its keyword\n",
    "    def map_to_keyword(description):\n",
    "        if isinstance(description, str):\n",
    "            words = description.split()\n",
    "            for word in words:\n",
    "                if word in keyword_dict:\n",
    "                    return word\n",
    "        return description\n",
    "\n",
    "    # Define a function to process each chunk of the DataFrame\n",
    "    def process_chunk(chunk):\n",
    "        chunk[\"BRAND\"] = chunk[column].apply(map_to_keyword)\n",
    "        grouped = chunk.groupby(group_columns + [\"BRAND\"]).sum(numeric_only=True).reset_index()\n",
    "        return grouped\n",
    "\n",
    "    # Process the DataFrame in chunks and concatenate the results\n",
    "    grouped_chunks = [process_chunk(chunk) for chunk in pd.read_csv(input_file, chunksize=chunksize)]\n",
    "    result = pd.concat(grouped_chunks)\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    result.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"The file has been processed and saved as {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_file = \"data_clean/shampoo_sale_data.csv\"\n",
    "output_file = \"data_clean/shampoo_sale_data_brand.csv\"\n",
    "column = \"DESCRIP\"\n",
    "group_columns = [\"STORE\", \"WEEK\"]\n",
    "\n",
    "time_function(group_by_keyword,input_file, output_file, column, keyword_brand_grouping, group_columns, chunksize=chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebe46b-732b-4aae-a460-5f59e1a568ed",
   "metadata": {},
   "source": [
    "### Might be old code, cant remember\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "457257bc-6b64-4301-b190-6b9a9068bb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[530, 'FLEX'], [514, 'RAVE'], [508, 'SUAVE'], [475, 'SUAVE'], [437, 'SUAVE']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[530, 514, 508, 475, 437], ['FLEX', 'RAVE', 'SUAVE', 'SUAVE', 'SUAVE']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def top_n_values_with_names(input_file, column_number, column_name, n=5):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Sort the DataFrame based on the values in the specified column_number\n",
    "    sorted_df = df.sort_values(by=column_number, ascending=False)\n",
    "\n",
    "    # Get the top N rows from the sorted DataFrame\n",
    "    top_n_rows = sorted_df.head(n)\n",
    "\n",
    "    # Create a list with the top N values and their corresponding names from column_name\n",
    "    result = top_n_rows[[column_number, column_name]].values.tolist()\n",
    "\n",
    "    return result\n",
    "\n",
    "input_file = 'data_clean/shampoo_sale_data_brand.csv'\n",
    "column_number = 'MOVE'\n",
    "column_name = 'BRAND'\n",
    "\n",
    "n = 5\n",
    "top_n_values = top_n_values_with_names(input_file, column_number,column_name, n)\n",
    "display(top_n_values)\n",
    "\n",
    "upc_list = [[row[i] for row in top_n_values] for i in range(2)] #top_n_values reordered to [[move...],[descrip...],[upc...]]\n",
    "display(upc_list)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1669a372-6aba-4737-b253-585357c6d92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gameb\\AppData\\Local\\Temp\\ipykernel_24376\\3095157030.py:78: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  grouped_df = df.groupby(['group', 'STORE', 'WEEK'], as_index=False).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been processed and saved as data_clean/shampoo_sale_data_test.csv\n",
      "process_and_group_rows took 1.94 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def process_and_group_rows(input_file, output_file, column, merge_on_columns):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Remove any symbols or spaces at the first position in the specified column\n",
    "    df[column] = df[column].apply(lambda x: re.sub(r'^[^\\w\\d]*', '', x))\n",
    "\n",
    "    # Extract keywords and create a dictionary of unique keywords and their corresponding values\n",
    "    keywords = set(df[column].apply(lambda x: x.split('_')[0].split()[0]))\n",
    "    keyword_groups = {}\n",
    "    for keyword in keywords:\n",
    "        values = []\n",
    "        for value in df[column].unique():\n",
    "            if value.startswith(keyword):\n",
    "                values.append(value)\n",
    "                continue\n",
    "            split_value = value.split()\n",
    "            for idx, word in enumerate(split_value):\n",
    "                if keyword == ' '.join(split_value[:idx + 1]):\n",
    "                    values.append(value)\n",
    "                    break\n",
    "        keyword_groups[keyword] = values\n",
    "\n",
    "    # Group rows based on the keyword and merge them under a new value named after the keyword\n",
    "    grouped_rows = []\n",
    "    for keyword, values in keyword_groups.items():\n",
    "        keyword_rows = df[df[column].isin(values)].copy()  # Create a copy of the filtered DataFrame\n",
    "        keyword_rows.loc[:, column] = keyword  # Use .loc[] to set the value\n",
    "        group_columns = merge_on_columns + [column]\n",
    "        merged_rows = keyword_rows.groupby(group_columns, as_index=False).sum(numeric_only=True)\n",
    "        grouped_rows.append(merged_rows)\n",
    "\n",
    "    # Concatenate grouped rows and save the result to a new CSV file\n",
    "    result = pd.concat(grouped_rows)\n",
    "    result.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"The file has been processed and saved as {output_file}\")\n",
    "\n",
    "\n",
    "input_file = \"data_clean/shampoo_sale_data.csv\"\n",
    "output_file = \"data_clean/shampoo_sale_data_test.csv\"\n",
    "column = \"DESCRIP\"\n",
    "merge_on_columns = [\"STORE\", \"WEEK\"]\n",
    "\n",
    "time_function(process_and_group_rows,input_file, output_file, column, merge_on_columns)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def get_common_start(strings):\n",
    "    #Find the common starting substring in a list of strings.\n",
    "    if not strings:\n",
    "        return ''\n",
    "    \n",
    "    # Sort the strings and compare the first and last one\n",
    "    strings = sorted(strings)\n",
    "    first = strings[0]\n",
    "    last = strings[-1]\n",
    "    common_start = []\n",
    "    \n",
    "    for char1, char2 in zip(first, last):\n",
    "        if char1 == char2:\n",
    "            common_start.append(char1)\n",
    "        else:\n",
    "            break  # Stop at the first mismatch\n",
    "    \n",
    "    return ''.join(common_start)\n",
    "\n",
    "def process_and_group_rows(input_file, output_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Clean the 'DESCRIP' column\n",
    "    df['DESCRIP'] = df['DESCRIP'].str.upper().str.replace(r'\\W+', ' ', regex=True)\n",
    "\n",
    "    # Extract first word as the main category\n",
    "    df['group'] = df['DESCRIP'].str.split().str[0]\n",
    "\n",
    "    # Group the DataFrame by 'group', 'STORE', and 'WEEK' and calculate the sum\n",
    "    grouped_df = df.groupby(['group', 'STORE', 'WEEK'], as_index=False).sum()\n",
    "\n",
    "    # Write the result to a new CSV file\n",
    "    grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"The file has been processed and saved as {output_file}\")\n",
    "\n",
    "\n",
    "input_file = \"data_clean/shampoo_sale_data.csv\"\n",
    "output_file = \"data_clean/shampoo_sale_data_test.csv\"\n",
    "column = \"DESCRIP\"\n",
    "merge_on_columns = [\"STORE\", \"WEEK\"]\n",
    "\n",
    "time_function(process_and_group_rows,input_file, output_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b7d7a-6264-4189-9c5d-79fa837112ac",
   "metadata": {},
   "source": [
    "### Weekly Raport\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcedcad3-4ee6-43dd-a63c-cd6081bbcc65",
   "metadata": {},
   "source": [
    "#### Filter and create a data set for a weekly report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29f1a778-695c-4416-a166-69857aab8af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The STORE with the highest frequency is 71, with a frequency of 901\n",
      "The WEEK with the highest frequency within the store 71 is 210, with a frequency of 56\n"
     ]
    }
   ],
   "source": [
    "def most_frequent_value(csv_file, store_column, week_column):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Calculate the frequency of each value in the specified store column\n",
    "    store_value_counts = df[store_column].value_counts()\n",
    "    most_frequent_store = store_value_counts.idxmax()\n",
    "    store_frequency = store_value_counts[most_frequent_store]\n",
    "\n",
    "    # Filter the DataFrame to keep only the most frequent store\n",
    "    store_df = df[df[store_column] == most_frequent_store]\n",
    "\n",
    "    # Calculate the frequency of each value in the specified week column\n",
    "    week_value_counts = store_df[week_column].value_counts()\n",
    "    most_frequent_week = week_value_counts.idxmax()\n",
    "    week_frequency = week_value_counts[most_frequent_week]\n",
    "    \n",
    "    print(f\"The {store_column} with the highest frequency is {most_frequent_store}, with a frequency of {store_frequency}\")\n",
    "    print(f\"The {week_column} with the highest frequency within the store {most_frequent_store} is {most_frequent_week}, with a frequency of {week_frequency}\")\n",
    "    return most_frequent_store, most_frequent_week\n",
    "\n",
    "# Example usage\n",
    "file_main = \"data_clean/shampoo_sale_data_brand.csv\"\n",
    "store_column = \"STORE\"\n",
    "week_column = \"WEEK\"\n",
    "most_frequent_store, most_frequent_week = most_frequent_value(file_main, store_column, week_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1d6a2f8-dbb2-4da2-a20d-caaecd332860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes with the encoding 'utf-8', file data_clean/week_shampoo_sale_data.csv now created\n",
      "load_and_filter_file took 0.20 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "def filter_select_week(chunk):\n",
    "    chunk_copy = chunk.copy()\n",
    "\n",
    "    # Modify the 'WEEK' column in the copied chunk\n",
    "    chunk_copy['WEEK'] = chunk_copy['WEEK'].astype(int)\n",
    "\n",
    "    filtered_chunk = chunk_copy[(chunk_copy['WEEK'] >= most_frequent_week-1) & (chunk_copy['WEEK'] <= most_frequent_week)]\n",
    "    return filtered_chunk\n",
    "\n",
    "def filter_select_store(chunk):\n",
    "    chunk_copy = chunk.copy()\n",
    "\n",
    "    # Modify the 'WEEK' column in the copied chunk\n",
    "    chunk_copy['STORE'] = chunk_copy['STORE'].astype(int)\n",
    "\n",
    "    filtered_chunk = chunk_copy[(chunk_copy['STORE'] == most_frequent_store)]\n",
    "    return filtered_chunk\n",
    "\n",
    "filter_func_list = [filter_select_store,filter_select_week]\n",
    "file_main = \"data_clean/shampoo_sale_data_brand.csv\"\n",
    "file_out = \"week_shampoo_sale_data.csv\"\n",
    "time_function(load_and_filter_file, file_main, folder_path_clean, filter_func_list, chunksize = chunk_size, new_file_name = file_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e657dfd-62e8-475d-b735-940768a5d7be",
   "metadata": {},
   "source": [
    "#### Create graph for weekly sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858337c-b488-40d6-a6a8-d7b06fe12d81",
   "metadata": {},
   "source": [
    "#### Compeare weekly sales, with last week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68ceca-a79d-456d-912f-d896e08cd2cd",
   "metadata": {},
   "source": [
    "#### Compeare sales area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55314e-7f62-4779-8db1-d6abfa104123",
   "metadata": {},
   "source": [
    "#### Compear brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb617e04-a29d-4468-9b94-a05c726673b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create graph for weekly sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af6b4e7-5d82-41e5-9185-3123b0285e6d",
   "metadata": {},
   "source": [
    "### Monthly Report\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842cde0-011a-4182-bbab-95f52862b40e",
   "metadata": {},
   "source": [
    "#### Filter and create a data set for a monthly report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "aca140d6-7224-4e4c-858e-8463fc71e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file has been grouped and saved as data_clean/month_shampoo_sale_data.csv\n",
      "group_by_week_and_convert_to_month took 1.36 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "def group_by_week_and_convert_to_month(input_file, output_file):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Group the DataFrame by \"WEEK\", \"UPC\", and \"STORE\"\n",
    "    grouped_df = df.groupby([\"WEEK\", \"BRAND\", \"STORE\"]).sum(numeric_only=True).reset_index()\n",
    "\n",
    "    # Function to convert week encoding to month\n",
    "    def week_to_month(week):\n",
    "        start_date = datetime.strptime(\"1992-12-31\", \"%Y-%m-%d\")\n",
    "        week_173_date = start_date + timedelta(weeks=(week - 173))\n",
    "        return week_173_date.month\n",
    "\n",
    "    # Convert \"WEEK\" to \"MONTHS\" using the week_to_month function\n",
    "    grouped_df[\"MONTHS\"] = grouped_df[\"WEEK\"].apply(week_to_month)\n",
    "\n",
    "    # Drop the \"WEEK\" column\n",
    "    grouped_df = grouped_df.drop(columns=[\"WEEK\"])\n",
    "    \n",
    "    # Reorder columns to move \"MONTHS\" to the right of \"STORE\"\n",
    "    new_column_order = ['BRAND', 'STORE', 'MONTHS'] + [col for col in grouped_df.columns if col not in ['BRAND', 'STORE', 'MONTHS']]\n",
    "    grouped_df = grouped_df.reindex(columns=new_column_order)\n",
    "\n",
    "    # Save the grouped and modified DataFrame to a new CSV file\n",
    "    grouped_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"The file has been grouped and saved as {output_file}\")\n",
    "    \n",
    "    \n",
    "input_file = \"data_clean/shampoo_sale_data_brand.csv\"\n",
    "output_file = \"data_clean/month_shampoo_sale_data.csv\"\n",
    "\n",
    "time_function(group_by_week_and_convert_to_month,input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c05574-300b-4a14-a57d-2b8c1172120a",
   "metadata": {},
   "source": [
    "#### Filter out to a monthly raport\n",
    "This is a monthly raport for the corporate management over all the stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04843bf4-73c0-4cc9-9be8-0577b084223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge_csv_files took 1.78 seconds to run.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c726e-0ddc-4678-a31e-c67f62a73810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
